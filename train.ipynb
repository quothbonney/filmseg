{"cells":[{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3450,"status":"ok","timestamp":1686952285319,"user":{"displayName":"Jackie D.V. Carson","userId":"03369580263560370573"},"user_tz":300},"id":"2xNIFcYOq9YX","outputId":"fc2a3038-b1c5-439f-e043-c6d6f6f2d6d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: kornia in /usr/local/lib/python3.10/dist-packages (0.6.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (23.1)\n","Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.1->kornia) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.1->kornia) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.1->kornia) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n"]}],"source":["!pip3 install kornia"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17843,"status":"ok","timestamp":1686952195651,"user":{"displayName":"Jackie D.V. Carson","userId":"03369580263560370573"},"user_tz":300},"id":"PETEJFjhqMqK","outputId":"0bc228d9-e16a-4c54-f540-cab5414ff064"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/FILM_DATABASE')\n","import paired_transforms as pt"],"metadata":{"id":"c26RMV-aLijh","executionInfo":{"status":"ok","timestamp":1686954635371,"user_tz":300,"elapsed":211,"user":{"displayName":"Jackie D.V. Carson","userId":"03369580263560370573"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","execution_count":17,"metadata":{"id":"WYTsLuxFrzuF","executionInfo":{"status":"ok","timestamp":1686952400568,"user_tz":300,"elapsed":6,"user":{"displayName":"Jackie D.V. Carson","userId":"03369580263560370573"}}},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","from PIL import Image\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","import kornia\n","import matplotlib.pyplot as plt\n","from torch import nn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w1DSEmY3qr5x"},"outputs":[],"source":["folder_path = \"/content/drive/MyDrive/FILM_DATABASE\""]},{"cell_type":"code","source":["class AugmentedDataset(Dataset):\n","    def __init__(self, base_dataset, transform, augment_factor):\n","        self.base_dataset = base_dataset\n","        self.transform = transform\n","        self.augment_factor = augment_factor\n","\n","    def __getitem__(self, index):\n","        base_index = index // self.augment_factor\n","        rh = pt.RandomHorizontalFlip()\n","        rv = pt.RandomVerticalFlip()\n","        rr = pt.RandomRotation(360)\n","        ca = pt.RandomAffine(45)\n","        cj = pt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)\n","        color_img, error_img = self.base_dataset[base_index]\n","        color_img, error_img = rh(color_img, error_img)\n","        color_img, error_img = rv(color_img, error_img)\n","        color_img, error_img = rr(color_img, error_img)\n","            #color_img, error_img = ra(color_img, error_img)\n","        color_img, error_img = cj(color_img, error_img)\n","\n","        color_img = transforms.ToTensor()(color_img)\n","        error_img = transforms.ToTensor()(error_img)\n","        color_img = kornia.color.rgb_to_hsv(color_img)\n","        return color_img, error_img\n","\n","    def __len__(self):\n","        return len(self.base_dataset) * self.augment_factor"],"metadata":{"id":"DlK1EZm2Bmho","executionInfo":{"status":"ok","timestamp":1686952287772,"user_tz":300,"elapsed":2,"user":{"displayName":"Jackie D.V. Carson","userId":"03369580263560370573"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self, color_dir, error_dir, window_size=256, blackness_threshold=10):\n","        self.window_size = window_size\n","        self.blackness_threshold = blackness_threshold\n","        self.min_augment = 5\n","        self.max_augment = 20\n","\n","\n","        # list of filenames\n","        filenames = os.listdir(color_dir)\n","\n","        self.color_imgs = []\n","        self.error_imgs = []\n","\n","        for filename in filenames:\n","            color_img = Image.open(os.path.join(color_dir, filename))\n","            error_img = Image.open(os.path.join(error_dir, filename))\n","            error_img = error_img.convert(\"L\")\n","\n","            step_size = self.window_size\n","            for i in range(0, 512, step_size):\n","                for j in range(0, 512, step_size):\n","                    sub_color_img = color_img.crop((i, j, i+step_size, j+step_size))\n","                    sub_error_img = error_img.crop((i, j, i+step_size, j+step_size))\n","\n","                    #if np.mean(np.array(sub_color_img)) > self.blackness_threshold:\n","                    self.color_imgs.append(sub_color_img)\n","                    self.error_imgs.append(sub_error_img)\n","\n","\n","    def __getitem__(self, index):\n","        return self.color_imgs[index], self.error_imgs[index]\n","\n","    def __len__(self):\n","        return len(self.color_imgs)\n","\n"],"metadata":{"id":"EQ1lu7gToMHt","executionInfo":{"status":"ok","timestamp":1686952291270,"user_tz":300,"elapsed":279,"user":{"displayName":"Jackie D.V. Carson","userId":"03369580263560370573"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import random_split\n","\n","# Constants\n","TRAIN_RATIO = 0.8\n","AUGMENT_FACTOR = 10\n","\n","# Use the custom dataset\n","folder_path = \"/content/drive/MyDrive/FILM_DATABASE\"\n","color_dir = os.path.join(folder_path, 'color_mask')\n","error_dir = os.path.join(folder_path, 'error_mask')\n","\n","# Create the full dataset\n","full_dataset = CustomDataset(color_dir, error_dir)\n","\n","# Compute the lengths of train/test subsets\n","train_len = int(len(full_dataset) * TRAIN_RATIO)\n","test_len = len(full_dataset) - train_len\n","\n","# Randomly split the dataset\n","train_dataset, test_dataset = random_split(full_dataset, [train_len, test_len])\n","\n","# Define your transformations\n","transform = transforms.Compose([\n","    # Add your transformations here\n","    transforms.ToTensor(),\n","])\n","\n","# Create the augmented dataset\n","augmented_train_dataset = AugmentedDataset(train_dataset, transform, augment_factor=AUGMENT_FACTOR)\n","augmented_test_dataset = AugmentedDataset(train_dataset, transform, augment_factor=1)\n","# Create DataLoaders\n","train_loader = DataLoader(augmented_train_dataset, batch_size=8, shuffle=True)\n","test_loader = DataLoader(augmented_test_dataset, batch_size=8, shuffle=False)\n"],"metadata":{"id":"a-8aljy8k26J","executionInfo":{"status":"ok","timestamp":1686952400568,"user_tz":300,"elapsed":69668,"user":{"displayName":"Jackie D.V. Carson","userId":"03369580263560370573"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["print(len(augmented_train_dataset))\n","print(len(augmented_test_dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t2HkZlO_TB80","executionInfo":{"status":"ok","timestamp":1686952424483,"user_tz":300,"elapsed":245,"user":{"displayName":"Jackie D.V. Carson","userId":"03369580263560370573"}},"outputId":"894aae3c-3ce2-40ed-ba98-0462cfd3e77b"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["2430\n","243\n"]}]},{"cell_type":"code","execution_count":20,"metadata":{"id":"w9HAw7QntTu4","executionInfo":{"status":"ok","timestamp":1686952427271,"user_tz":300,"elapsed":336,"user":{"displayName":"Jackie D.V. Carson","userId":"03369580263560370573"}}},"outputs":[],"source":["class DoubleConv(nn.Module):\n","    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, mid_channels=None):\n","        super().__init__()\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(mid_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","class Down(nn.Module):\n","    \"\"\"Downscaling with maxpool then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.maxpool_conv(x)\n","\n","class Up(nn.Module):\n","    \"\"\"Upscaling then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, bilinear=True):\n","        super().__init__()\n","\n","        if bilinear:\n","            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","        else:\n","            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n","\n","        self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","\n","        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n","        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n","\n","        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n","                        diffY // 2, diffY - diffY // 2])\n","\n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n","\n","class OutConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(OutConv, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class UNet(nn.Module):\n","    def __init__(self, n_channels, n_classes, bilinear=True):\n","        super(UNet, self).__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.bilinear = bilinear\n","\n","        self.inc = DoubleConv(n_channels, 64)\n","        self.down1 = Down(64, 128)\n","        self.down2 = Down(128, 256)\n","        self.down3 = Down(256, 512)\n","        self.down4 = Down(512, 512)\n","        self.up1 = Up(1024, 256, bilinear)\n","        self.up2 = Up(512, 128, bilinear)\n","        self.up3 = Up(256, 64, bilinear)\n","        self.up4 = Up(128, 64, bilinear)\n","        self.outc = OutConv(64, n_classes)\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        logits = self.outc(x)\n","        return logits\n","\n","# Create an instance of the model\n","model = UNet(n_channels=3, n_classes=1)"]},{"cell_type":"code","source":["# Run this before retraining (clear Torch GPURAM cache)\n","with torch.no_grad():\n","    torch.cuda.empty_cache()"],"metadata":{"id":"TOHioQBNVmLI","executionInfo":{"status":"ok","timestamp":1686952420251,"user_tz":300,"elapsed":219,"user":{"displayName":"Jackie D.V. Carson","userId":"03369580263560370573"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P-UbbMZ8s8fK"},"outputs":[],"source":["import torch.optim as optim\n","losses = []\n","\n","# If a GPU is available, move the model to GPU\n","model = model.to(\"cuda\")\n","\n","# Initialize the optimizer and loss function\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","criterion = torch.nn.BCEWithLogitsLoss()\n","\n","# Number of epochs\n","n_epochs = 7\n","\n","# Training loop\n","for epoch in range(2, n_epochs):\n","    model.train()  # set the model to training mode\n","    if epoch == 5:\n","      new_lr = 0.0003\n","      for param_group in optimizer.param_groups:\n","        param_group['lr'] = new_lr\n","\n","# Update learning rate for all parameter groups\n","\n","\n","    for i, (inputs, targets) in enumerate(train_loader):\n","        # If a GPU is available, move data and targets to GPU\n","        inputs = inputs.to(\"cuda\")\n","        targets = targets.to(\"cuda\")\n","\n","        # Clear gradients\n","        optimizer.zero_grad()\n","\n","        # Forward propagation\n","        targets = targets.squeeze(0)\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets.float())\n","\n","        # Backward propagation\n","        loss.backward()\n","\n","        # Update weights\n","        optimizer.step()\n","        losses.append(loss.item())\n","        # Print loss every 10 steps\n","        if i % 20 == 0:\n","          print(f\"Epoch: {epoch}, Step: {i}, Loss: {loss.item()}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"liRkgq5VtI7f"},"outputs":[],"source":["with torch.no_grad():\n","    for i, (inputs, targets) in enumerate(test_loader):\n","        # If a GPU is available, move data and targets to GPU\n","        inputs = inputs.to(\"cuda\")\n","        targets = targets.to(\"cuda\")\n","\n","        # Clear gradients\n","        optimizer.zero_grad()\n","\n","        # Forward propagation\n","        targets = targets.squeeze(0)\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets.float())\n","\n","        # Print loss every 10 steps\n","        print(loss.item())"]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"/content/drive/MyDrive/tudel_unet.1\")"],"metadata":{"id":"8v1neTWb3tmd"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyMpXOaoPHG28FmVm8oQzr3x"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}